{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Teacher-Model\" data-toc-modified-id=\"Teacher-Model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Teacher Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-transformation-for-dense-features\" data-toc-modified-id=\"Simple-transformation-for-dense-features-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Simple transformation for dense features</a></span></li><li><span><a href=\"#Set-hashing-space-for-each-sparse-field\" data-toc-modified-id=\"Set-hashing-space-for-each-sparse-field-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Set hashing space for each sparse field</a></span></li><li><span><a href=\"#Generate-input-data-for-model\" data-toc-modified-id=\"Generate-input-data-for-model-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Generate input data for model</a></span></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Model\" data-toc-modified-id=\"Define-Model-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Define Model</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Train</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Soft-targets\" data-toc-modified-id=\"Soft-targets-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Soft-targets</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.dcn import DCN\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREFIX = \"../../../data/criteo/\"\n",
    "\n",
    "TRAIN_DATA = os.path.join(DATA_PREFIX, 'train.csv')\n",
    "TEST_DATA = os.path.join(DATA_PREFIX, 'test.csv')\n",
    "TEST_LABELS_DATA = os.path.join(DATA_PREFIX, 'test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=dict([(col, col[1:] if col[0] == '_' else col) for col in data.columns]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10',\n",
       "       'c11', 'c12', 'c13', 'c14', 'c15', 'c16', 'c17', 'c18', 'c19', 'c20',\n",
       "       'c21', 'c22', 'c23', 'c24', 'c25', 'c26', 'c27', 'c28', 'c29', 'c30',\n",
       "       'c31', 'c32', 'c33', 'c34', 'c35', 'c36', 'c37', 'c38', 'c39', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 26)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_features = ['c{}'.format(i) for i in range(1, 14)]\n",
    "sparse_features = ['c{}'.format(i) for i in range(14, 40)]\n",
    "\n",
    "len(dense_features), len(sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "target = ['c0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c14', 1445),\n",
       " ('c15', 556),\n",
       " ('c16', 1130758),\n",
       " ('c17', 360209),\n",
       " ('c18', 304),\n",
       " ('c19', 21),\n",
       " ('c20', 11845),\n",
       " ('c21', 631),\n",
       " ('c22', 3),\n",
       " ('c23', 49223),\n",
       " ('c24', 5194),\n",
       " ('c25', 985420),\n",
       " ('c26', 3157),\n",
       " ('c27', 26),\n",
       " ('c28', 11588),\n",
       " ('c29', 715441),\n",
       " ('c30', 10),\n",
       " ('c31', 4681),\n",
       " ('c32', 2029),\n",
       " ('c33', 4),\n",
       " ('c34', 870796),\n",
       " ('c35', 17),\n",
       " ('c36', 15),\n",
       " ('c37', 87605),\n",
       " ('c38', 84),\n",
       " ('c39', 58187)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(feat, data[feat].nunique()) for feat in sparse_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple transformation for dense features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hashing space for each sparse field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=1000, embedding_dim=4, use_hash=True, dtype='string') \n",
    "                          for feat in sparse_features] + \\\n",
    "                        [DenseFeat(feat, 1,) \n",
    "                         for feat in dense_features]\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2931944, 366493, 366494)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "validation, test = train_test_split(test, test_size=0.5, shuffle=False)\n",
    "\n",
    "len(train), len(validation), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model_input(df):\n",
    "    return {name: df[name] for name in feature_names}\n",
    "\n",
    "\n",
    "train_model_input = gen_model_input(train)\n",
    "validation_model_input = gen_model_input(validation)\n",
    "test_model_input = gen_model_input(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN(linear_feature_columns, dnn_feature_columns, cross_num=2,\n",
    "            dnn_hidden_units=(128, 128), l2_reg_linear=3e-05, l2_reg_embedding=3e-05,\n",
    "            l2_reg_cross=3e-05, l2_reg_dnn=0, init_std=0.0001, seed=1024, \n",
    "            dnn_dropout=0.2, dnn_use_bn=True, dnn_activation='relu', task='binary')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\"} values), <class 'NoneType'>\n",
      "Train on 2931944 samples, validate on 366493 samples\n",
      "Epoch 1/5\n",
      "2931456/2931944 [============================>.] - ETA: 0s - loss: 0.4863 - binary_crossentropy: 0.4819\n",
      "Epoch 00001: loss improved from inf to 0.48632, saving model to best_model.hdf5\n",
      "2931944/2931944 [==============================] - 185s 63us/sample - loss: 0.4863 - binary_crossentropy: 0.4819 - val_loss: 0.4836 - val_binary_crossentropy: 0.4771\n",
      "Epoch 2/5\n",
      "2931456/2931944 [============================>.] - ETA: 0s - loss: 0.4809 - binary_crossentropy: 0.4737\n",
      "Epoch 00002: loss improved from 0.48632 to 0.48089, saving model to best_model.hdf5\n",
      "2931944/2931944 [==============================] - 187s 64us/sample - loss: 0.4809 - binary_crossentropy: 0.4737 - val_loss: 0.4799 - val_binary_crossentropy: 0.4727\n",
      "Epoch 3/5\n",
      "2931712/2931944 [============================>.] - ETA: 0s - loss: 0.4784 - binary_crossentropy: 0.4708\n",
      "Epoch 00003: loss improved from 0.48089 to 0.47840, saving model to best_model.hdf5\n",
      "2931944/2931944 [==============================] - 185s 63us/sample - loss: 0.4784 - binary_crossentropy: 0.4708 - val_loss: 0.4793 - val_binary_crossentropy: 0.4719\n",
      "Epoch 4/5\n",
      "2931712/2931944 [============================>.] - ETA: 0s - loss: 0.4771 - binary_crossentropy: 0.4694\n",
      "Epoch 00004: loss improved from 0.47840 to 0.47711, saving model to best_model.hdf5\n",
      "2931944/2931944 [==============================] - 186s 63us/sample - loss: 0.4771 - binary_crossentropy: 0.4694 - val_loss: 0.4773 - val_binary_crossentropy: 0.4697\n",
      "Epoch 5/5\n",
      "2931200/2931944 [============================>.] - ETA: 0s - loss: 0.4763 - binary_crossentropy: 0.4684\n",
      "Epoch 00005: loss improved from 0.47711 to 0.47627, saving model to best_model.hdf5\n",
      "2931944/2931944 [==============================] - 184s 63us/sample - loss: 0.4763 - binary_crossentropy: 0.4684 - val_loss: 0.4766 - val_binary_crossentropy: 0.4690\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values, \n",
    "                    batch_size=256, epochs=5, verbose=1, use_multiprocessing=True,\n",
    "                    callbacks=[checkpoint],\n",
    "                    validation_data=(validation_model_input, validation[target].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join(DATA_PREFIX, 'DCN_w.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model size**\n",
    "\n",
    "примерно 5MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\"} values), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test LogLoss 0.4737\n",
      "test AUC 0.7808\n"
     ]
    }
   ],
   "source": [
    "print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\"} values), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "preds_train = model.predict(train_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [p[0] for p in preds_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id': train['id'], 'prob': probs}) \\\n",
    "    .to_csv(os.path.join(DATA_PREFIX, 'soft_targets.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
